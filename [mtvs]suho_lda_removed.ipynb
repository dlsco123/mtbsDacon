{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리내 변수 제거\n",
    "\n",
    "all = [var for var in globals() if var[0] != \"_\"]   # globals() 목록의 첫글자가 _ 로 시작하지 않는 자료의 리스트만 가져와서\n",
    "for var in all:\n",
    "    del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xgboost\n",
    "#%pip install wordcloud\n",
    "#%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "## for data\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk## for language detection\n",
    "\n",
    "# 박스 출력\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('c:/data/project/train.csv')\n",
    "test = pd.read_csv('c:/data/project/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    1649\n",
       "0     829\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['first_party_winner'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[['facts', 'first_party_winner']]\n",
    "df_target = df[['first_party_winner']]\n",
    "df_nlp = df[['facts']]\n",
    "df_nlp1 = pd.DataFrame(df_nlp, columns=['facts'])\n",
    "df_nlp1['facts'] = df_nlp1['facts'].str.replace(r'<[^<>]*>', '', regex=True) # 특수 문자 제거\n",
    "df_nlp1['facts'] = df_nlp1['facts'].str.replace(r'\\d', '', regex=True)  # 숫자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_nlp1['facts']\n",
    "# print(corpus.str.cat(sep=\" \")) # 인덱스의 요소들 서로 잇기\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 깨끗하게 만드는 함수\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=True, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    import re\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list # 역토큰화, 벡터화 하기 위해서\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 깨끗하게 만드는 함수 사용\n",
    "df_nlp1[\"facts_clean\"] = df_nlp1[\"facts\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "corpus_stopwords = df_nlp1[\"facts_clean\"]\n",
    "lst_tokens_stopwords = nltk.tokenize.word_tokenize(corpus_stopwords.str.cat(sep=\" \"))\n",
    "stop_words=[]\n",
    "# 갯수가 하나만 있는 단어들 제거\n",
    "for word, freq in nltk.FreqDist(lst_tokens_stopwords).most_common():\n",
    "    if freq == 1:\n",
    "        #print(word)\n",
    "        stop_words.append(word)\n",
    "\n",
    "df_nlp1[\"facts_clean\"] = df_nlp1[\"facts_clean\"] .apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False, lst_stopwords=stop_words))\n",
    "# y값 포함해서 하나의 프레임으로 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facts_clean</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>june phil st amant candidate public office mad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ramon nelson riding bike suffered lethal blow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alabama state court convicted billy joe magwoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>victor linkletter convicted state court eviden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>april selma alabama intruder broke apartment d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>congress amended clean air act energy policy a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>alliance bond fund inc investment fund purchas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>district court sentenced manuel peguero month ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>march st cyr lawful permanent resident pled gu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>herbert owns patent system track clothing dryc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2478 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            facts_clean  first_party_winner\n",
       "0     june phil st amant candidate public office mad...                   1\n",
       "1     ramon nelson riding bike suffered lethal blow ...                   0\n",
       "2     alabama state court convicted billy joe magwoo...                   1\n",
       "3     victor linkletter convicted state court eviden...                   0\n",
       "4     april selma alabama intruder broke apartment d...                   1\n",
       "...                                                 ...                 ...\n",
       "2473  congress amended clean air act energy policy a...                   1\n",
       "2474  alliance bond fund inc investment fund purchas...                   1\n",
       "2475  district court sentenced manuel peguero month ...                   0\n",
       "2476  march st cyr lawful permanent resident pled gu...                   0\n",
       "2477  herbert owns patent system track clothing dryc...                   0\n",
       "\n",
       "[2478 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델링 전 완전 데이터\n",
    "df_nlp2 = pd.concat([df_nlp1,df_target['first_party_winner']],axis=1, join='inner')\n",
    "df_nlp2 = df_nlp2.drop(columns='facts')\n",
    "df_nlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF / 전체 문서들 중에서 빈도수가 높으면 가중치가 낮고, 특정 문서에서 빈도수가 높으면 가중치를 높게 줌\n",
    "\n",
    "# vectorizeTF = TfidfVectorizer()\n",
    "# count_matrix_tf = vectorizeTF.fit_transform(df_nlp2['facts_clean'])\n",
    "# data_final = count_matrix_tf.toarray()\n",
    "# data_final = pd.DataFrame(data=data_final, columns=vectorizeTF.get_feature_names_out())\n",
    "# data_final = pd.concat([data_final,df_nlp2[\"first_party_winner\"]],axis=1,join='inner')\n",
    "# terms = vectorizeTF.get_feature_names_out()\n",
    "# data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카운터 벡터라이즈 / 전체 문서에서 빈도수가 높으면 가중치가 높음\n",
    "   \n",
    "# vectorizeCV=CountVectorizer()\n",
    "# count_matrix = vectorizeCV.fit_transform(df_nlp2['facts_clean'])\n",
    "# count_array = count_matrix.toarray()\n",
    "# data_final = pd.DataFrame(data=count_array,columns = vectorizeCV.get_feature_names_out())\n",
    "# data_final = pd.concat([data_final,df_nlp2[\"first_party_winner\"]],axis=1,join='inner')\n",
    "# terms = vectorizeCV.get_feature_names_out()\n",
    "# data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseVec(vec,nlp,testSet=False):\n",
    "    \"\"\"\n",
    "        벡터 고르는 함수입니다.\n",
    "        vec 파라미터로 tf-idf이면 'tf', count-vector는 'cv'.\n",
    "        nlp는 깨끗하게 전처리된 데이터셋을 보내주세요.\n",
    "    \"\"\"\n",
    "    global terms, vectorize\n",
    "\n",
    "    if testSet == True: # 테스트 셋일 경우, 새로운 인스턴스를 부여하면 안되니까\n",
    "        count_matrix = vectorize.transform(nlp['facts_clean'])\n",
    "    \n",
    "    if vec == 'tf' and testSet == False:\n",
    "        #TFIDF / 전체 문서들 중에서 빈도수가 높으면 가중치가 낮고, 특정 문서에서 빈도수가 높으면 가중치를 높게 줌\n",
    "        vectorize = TfidfVectorizer()\n",
    "        count_matrix = vectorize.fit_transform(nlp['facts_clean'])\n",
    "\n",
    "    if vec == 'cv'and testSet == False:\n",
    "        # 카운터 벡터라이즈 / 전체 문서에서 빈도수가 높으면 가중치가 높음\n",
    "        vectorize=CountVectorizer()\n",
    "        count_matrix = vectorize.fit_transform(nlp['facts_clean'])\n",
    "\n",
    "    count_array = count_matrix.toarray()\n",
    "    data_final = pd.DataFrame(data=count_array,columns = vectorize.get_feature_names_out())\n",
    "    data_final = pd.concat([data_final,nlp[\"first_party_winner\"]],axis=1,join='inner')\n",
    "    terms = vectorize.get_feature_names_out()\n",
    "    testSet = False\n",
    "    return data_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = chooseVec('tf', df_nlp2) # td-idf : 'tf' / counter vector : 'cv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(data_final.drop(columns=['first_party_winner']), data_final['first_party_winner'], test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# lda = LatentDirichletAllocation(n_components=200, random_state=1) # n_component : 토픽 갯수\n",
    "# pd.DataFrame(lda.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 쪼개기\n",
    "def splitData(data):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['first_party_winner']), data['first_party_winner'], test_size=0.3,random_state=0)\n",
    "\n",
    "# LDA 학습\n",
    "# def ldaTraining(data, n):\n",
    "#     \"\"\"\n",
    "#         X_trian과 lDA 적용 후 확인할 데이터 n개\n",
    "#     \"\"\"\n",
    "#     # LDA 학습, 단어의 의미구조 파악\n",
    "#     # 30초정도 걸림\n",
    "#     global lda, X_train\n",
    "#     from sklearn.decomposition import LatentDirichletAllocation\n",
    "#     lda = LatentDirichletAllocation(n_components=200, random_state=1) # n_component : 토픽 갯수\n",
    "#     lda_data = lda.fit_transform(data)\n",
    "#     X_train = pd.DataFrame(data=lda_data)\n",
    "#     # LDA 학습 결과 보기\n",
    "#     for idx, topic in enumerate(lda.components_):\n",
    "#         print(\"Topic %d:\" % (idx+1), [(terms[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "\n",
    "# 모델 만들기\n",
    "def modeling(params):\n",
    "    \"\"\"\n",
    "        XGB 학습 모델 함수.\n",
    "        XGB 파라미터를 받습니다.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    model = XGBClassifier(**params) # **param_dict\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model.get_params())\n",
    "\n",
    "# 예측하기\n",
    "def prediction(data):\n",
    "    \"\"\"\n",
    "        테스트 데이터 예측하기\n",
    "    \"\"\"\n",
    "    # lda_data_t = pd.DataFrame(data=lda.transform(data)) # 테스트 lda 적용\n",
    "    y_pred = model.predict(data)\n",
    "    y_pred_df = pd.DataFrame(data=y_pred, columns=['data'])\n",
    "    display(y_pred_df.value_counts())\n",
    "\n",
    "    # 평가\n",
    "    from sklearn.metrics import f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred) # 정확도\n",
    "    print(\"Accuracy : %.2f%%\" % (accuracy * 100.0)) \n",
    "    print('f1_score :', f1_score(y_test, y_pred)) # 정밀도와 재현율의 조화평균\n",
    "    print(\"model score (X_train, y_train):\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.6, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': 0.3, 'subsample': 0.9, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "def fullTraining(data):\n",
    "    # 훈련\n",
    "    splitData(data)\n",
    "    # ldaTraining(X_train, 5)\n",
    "\n",
    "    # XG부스트 모델 적용하기\n",
    "    default_gs_params = {'learning_rate': 0.6, 'max_depth': 9, 'scale_pos_weight': 0.3, 'subsample': 0.9}\n",
    "    modeling(default_gs_params)\n",
    "\n",
    "fullTraining(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data\n",
       "1       516\n",
       "0       228\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 58.20%\n",
      "f1_score : 0.6886886886886887\n",
      "model core (X_train, y_train): 0.581989247311828\n"
     ]
    }
   ],
   "source": [
    "# 테스트 적용\n",
    "prediction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드서치로 최적의 파라미터 값 찾아보기\n",
    "\n",
    "def findParam():\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_xgb = {\"scale_pos_weight\": [0.3],\n",
    "                 \"learning_rate\":[0.3, 0.6, 0.9], # 0~1 높을 수록 과적합 된다, 만약에 값이 낮으면 n_estimators를 높여야 과적합이 방지\n",
    "                 \"max_depth\":[3,5,7,9], # 보통 3~10, 높을 수록 과적합\n",
    "                 \"subsample\":[0.5, 0.7, 0.9] # 학습에 사용하는 샘플링 비율 0.5 ~ 1, 높을 수록 과적합\n",
    "                }    \n",
    "\n",
    "    gscv_xgb = GridSearchCV (estimator = model, param_grid = param_xgb, scoring ='f1', cv = 3, refit=True, n_jobs=1, verbose=2)\n",
    "    gscv_xgb.fit(X_train, y_train)\n",
    "    gs_params= gscv_xgb.best_params_\n",
    "    print('XGB 파라미터: ', gs_params)\n",
    "    print('XGB 예측 정확도: {:.4f}'.format(gscv_xgb.best_score_))\n",
    "\n",
    "    return gs_params\n",
    "# 그리드서치 후 파라미터 적용해서 모델링 학습 다시 하기\n",
    "# modeling(findParam())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트csv 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.DataFrame(test['facts'])\n",
    "dfTest['facts'] = dfTest['facts'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "dfTest['facts'] = dfTest['facts'].str.replace(r'\\d', '', regex=True)  # 숫자 제거\n",
    "dfTest[\"facts_clean\"] = dfTest[\"facts\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "dfTest['first_party_winner'] = np.zeros(len(dfTest)).astype(int)\n",
    "dfTest = dfTest.drop(columns='facts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facts_clean</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bail reform act allowed federal court detain a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lexecon inc defendant class action lawsuit usc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fox television station broadcast billboard mus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial armed robbery federally insured saving l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>magistrate judge issued warrant authorizing se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>according executive order signed president geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>section clean air act caa requires environment...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>linda matteo john madigan created plan utilizi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>north carolina board agriculture adopted regul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>august dr paul berheldt stabbed death kitchen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            facts_clean  first_party_winner\n",
       "0     bail reform act allowed federal court detain a...                   0\n",
       "1     lexecon inc defendant class action lawsuit usc...                   0\n",
       "2     fox television station broadcast billboard mus...                   0\n",
       "3     trial armed robbery federally insured saving l...                   0\n",
       "4     magistrate judge issued warrant authorizing se...                   0\n",
       "...                                                 ...                 ...\n",
       "1235  according executive order signed president geo...                   0\n",
       "1236  section clean air act caa requires environment...                   0\n",
       "1237  linda matteo john madigan created plan utilizi...                   0\n",
       "1238  north carolina board agriculture adopted regul...                   0\n",
       "1239  august dr paul berheldt stabbed death kitchen ...                   0\n",
       "\n",
       "[1240 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>aa</th>\n",
       "      <th>aacw</th>\n",
       "      <th>aai</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abatement</th>\n",
       "      <th>...</th>\n",
       "      <th>zj</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoneofinterests</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurko</th>\n",
       "      <th>zurkos</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 10800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       __   aa  aacw  aai  aar  aaron   ab  abandoned  abandonment  abatement  \\\n",
       "0     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "2     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "3     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "4     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "...   ...  ...   ...  ...  ...    ...  ...        ...          ...        ...   \n",
       "1235  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1236  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1237  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1238  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1239  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "\n",
       "      ...   zj  zone  zoned  zoneofinterests  zoning  zuni  zurich  zurko  \\\n",
       "0     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "2     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "3     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "4     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "...   ...  ...   ...    ...              ...     ...   ...     ...    ...   \n",
       "1235  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1236  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1237  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1238  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1239  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "\n",
       "      zurkos  first_party_winner  \n",
       "0        0.0                   0  \n",
       "1        0.0                   0  \n",
       "2        0.0                   0  \n",
       "3        0.0                   0  \n",
       "4        0.0                   0  \n",
       "...      ...                 ...  \n",
       "1235     0.0                   0  \n",
       "1236     0.0                   0  \n",
       "1237     0.0                   0  \n",
       "1238     0.0                   0  \n",
       "1239     0.0                   0  \n",
       "\n",
       "[1240 rows x 10800 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터 고르기\n",
    "data_final_test = chooseVec('tf', dfTest, True)\n",
    "data_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>aa</th>\n",
       "      <th>aacw</th>\n",
       "      <th>aai</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abatement</th>\n",
       "      <th>...</th>\n",
       "      <th>zj</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoned</th>\n",
       "      <th>zoneofinterests</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurko</th>\n",
       "      <th>zurkos</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 10800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       __   aa  aacw  aai  aar  aaron   ab  abandoned  abandonment  abatement  \\\n",
       "0     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "2     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "3     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "4     0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "...   ...  ...   ...  ...  ...    ...  ...        ...          ...        ...   \n",
       "1235  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1236  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1237  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1238  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "1239  0.0  0.0   0.0  0.0  0.0    0.0  0.0        0.0          0.0        0.0   \n",
       "\n",
       "      ...   zj  zone  zoned  zoneofinterests  zoning  zuni  zurich  zurko  \\\n",
       "0     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "2     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "3     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "4     ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "...   ...  ...   ...    ...              ...     ...   ...     ...    ...   \n",
       "1235  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1236  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1237  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1238  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "1239  ...  0.0   0.0    0.0              0.0     0.0   0.0     0.0    0.0   \n",
       "\n",
       "      zurkos  first_party_winner  \n",
       "0        0.0                   0  \n",
       "1        0.0                   0  \n",
       "2        0.0                   0  \n",
       "3        0.0                   0  \n",
       "4        0.0                   0  \n",
       "...      ...                 ...  \n",
       "1235     0.0                   0  \n",
       "1236     0.0                   0  \n",
       "1237     0.0                   0  \n",
       "1238     0.0                   0  \n",
       "1239     0.0                   0  \n",
       "\n",
       "[1240 rows x 10800 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitData(data_final_test)\n",
    "def testSplit(data):\n",
    "    global X_test, y_test\n",
    "    X_test = data.drop(columns=['first_party_winner'])\n",
    "    y_test = data['first_party_winner']\n",
    "testSplit(data_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPredict():\n",
    "    global predcsv, X_test\n",
    "    \n",
    "    # X_test = dataTest.drop(columns=['first_party_winner'])\n",
    "    # y_test = dataTest['first_party_winner']\n",
    "    # X_test = pd.DataFrame(data=lda.transform(X_test))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    predcsv = pd.DataFrame(y_pred_test, columns=['first_party_winner'])\n",
    "\n",
    "testPredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission():\n",
    "    submit = pd.read_csv('C:/data/project/sample_submission.csv')\n",
    "    submit['first_party_winner'] = predcsv\n",
    "    submit.to_csv('./sample_submission.csv', index=False)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1                     878\n",
       "0                     362\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcsv.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "submission()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀프 트레이닝"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def findGoodSample():\n",
    "    # 데이터 준비\n",
    "    proba = model.predict_proba(X_test)\n",
    "    proba_df = pd.DataFrame(data=proba, columns=['proba_0','proba_1'])\n",
    "\n",
    "    # 0보다 1의 확률이 높은 데이터 추출\n",
    "    proba_higher = proba_df[proba_df['proba_1'] > proba_df['proba_0']]\n",
    "    # 1이 될 확률이 90프로 이상인 자료의 인덱스\n",
    "    proba_higher_index = proba_higher[proba_higher['proba_1'] > 0.9].index.to_list()\n",
    "\n",
    "    print('추가되는 샘플 갯수 :',len(proba_higher_index))\n",
    "    display(proba_higher_index)\n",
    "    # 인덱스 번호 리턴\n",
    "    return proba_higher_index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfTest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def concatSample():\n",
    "    newIdx = findGoodSample()\n",
    "    filtered = dfTest.loc[dfTest.index.isin(newIdx)].drop(columns='facts')\n",
    "    filtered['first_party_winner'] = 1\n",
    "    dataTest = pd.concat([df_nlp2,filtered]).reset_index(drop=True)\n",
    "    print('기존 샘플 갯수 :', len(df_nlp2))\n",
    "    print('새로운 프레임 데이터 갯수 :', len(dataTest))\n",
    "    return dataTest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_final_test = chooseVec('tf', concatSample(), True)\n",
    "fullTraining(data_final_test)\n",
    "prediction(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testPredict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predcsv.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
