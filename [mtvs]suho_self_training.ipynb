{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리내 변수 제거\n",
    "\n",
    "all = [var for var in globals() if var[0] != \"_\"]   # globals() 목록의 첫글자가 _ 로 시작하지 않는 자료의 리스트만 가져와서\n",
    "for var in all:\n",
    "    del globals()[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xgboost\n",
    "#%pip install wordcloud\n",
    "#%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "## for data\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk## for language detection\n",
    "\n",
    "# 박스 출력\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>first_party</th>\n",
       "      <th>second_party</th>\n",
       "      <th>facts</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Phil A. St. Amant</td>\n",
       "      <td>Herman A. Thompson</td>\n",
       "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Stephen Duncan</td>\n",
       "      <td>Lawrence Owens</td>\n",
       "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Billy Joe Magwood</td>\n",
       "      <td>Tony Patterson, Warden, et al.</td>\n",
       "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Linkletter</td>\n",
       "      <td>Walker</td>\n",
       "      <td>Victor Linkletter was convicted in state court...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>William Earl Fikes</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID         first_party                    second_party  \\\n",
       "0  TRAIN_0000   Phil A. St. Amant              Herman A. Thompson   \n",
       "1  TRAIN_0001      Stephen Duncan                  Lawrence Owens   \n",
       "2  TRAIN_0002   Billy Joe Magwood  Tony Patterson, Warden, et al.   \n",
       "3  TRAIN_0003          Linkletter                          Walker   \n",
       "4  TRAIN_0004  William Earl Fikes                         Alabama   \n",
       "\n",
       "                                               facts  first_party_winner  \n",
       "0  On June 27, 1962, Phil St. Amant, a candidate ...                   1  \n",
       "1  Ramon Nelson was riding his bike when he suffe...                   0  \n",
       "2  An Alabama state court convicted Billy Joe Mag...                   1  \n",
       "3  Victor Linkletter was convicted in state court...                   0  \n",
       "4  On April 24, 1953 in Selma, Alabama, an intrud...                   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('c:/data/project/train.csv')\n",
    "test = pd.read_csv('c:/data/project/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1    1649\n",
       "0     829\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['first_party_winner'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[['facts', 'first_party_winner']]\n",
    "df_target = df[['first_party_winner']]\n",
    "df_nlp = df[['facts']]\n",
    "df_nlp1 = pd.DataFrame(df_nlp, columns=['facts'])\n",
    "df_nlp1['facts'] = df_nlp1['facts'].str.replace(r'<[^<>]*>', '', regex=True) # 특수 문자 제거\n",
    "df_nlp1['facts'] = df_nlp1['facts'].str.replace(r'\\d', '', regex=True)  # 숫자 제거\n",
    "\n",
    "dfTest = pd.DataFrame(test['facts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_nlp1['facts']\n",
    "# print(corpus.str.cat(sep=\" \")) # 인덱스의 요소들 서로 잇기\n",
    "lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\n",
    "ps = nltk.stem.porter.PorterStemmer()\n",
    "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 깨끗하게 만드는 함수\n",
    "\n",
    "def utils_preprocess_text(text, flg_stemm=True, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    import re\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list # 역토큰화, 벡터화 하기 위해서\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 깨끗하게 만드는 함수 사용\n",
    "df_nlp1[\"facts_clean\"] = df_nlp1[\"facts\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "corpus_stopwords = df_nlp1[\"facts_clean\"]\n",
    "lst_tokens_stopwords = nltk.tokenize.word_tokenize(corpus_stopwords.str.cat(sep=\" \"))\n",
    "stop_words=[]\n",
    "for word, freq in nltk.FreqDist(lst_tokens_stopwords).most_common():\n",
    "    if freq == 1:\n",
    "        #print(word)\n",
    "        stop_words.append(word)\n",
    "\n",
    "df_nlp1[\"facts_clean\"] = df_nlp1[\"facts_clean\"] .apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False, lst_stopwords=stop_words))\n",
    "# y값 포함해서 하나의 프레임으로 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링 전 완전 데이터\n",
    "df_nlp2 = pd.concat([df_nlp1,df_target['first_party_winner']],axis=1, join='inner')\n",
    "df_nlp2 = df_nlp2.drop(columns='facts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF / 전체 문서들 중에서 빈도수가 높으면 가중치가 낮고, 특정 문서에서 빈도수가 높으면 가중치를 높게 줌\n",
    "\n",
    "# vectorizeTF = TfidfVectorizer()\n",
    "# count_matrix_tf = vectorizeTF.fit_transform(df_nlp2['facts_clean'])\n",
    "# data_final = count_matrix_tf.toarray()\n",
    "# data_final = pd.DataFrame(data=data_final, columns=vectorizeTF.get_feature_names_out())\n",
    "# data_final = pd.concat([data_final,df_nlp2[\"first_party_winner\"]],axis=1,join='inner')\n",
    "# terms = vectorizeTF.get_feature_names_out()\n",
    "# data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카운터 벡터라이즈 / 전체 문서에서 빈도수가 높으면 가중치가 높음\n",
    "   \n",
    "# vectorizeCV=CountVectorizer()\n",
    "# count_matrix = vectorizeCV.fit_transform(df_nlp2['facts_clean'])\n",
    "# count_array = count_matrix.toarray()\n",
    "# data_final = pd.DataFrame(data=count_array,columns = vectorizeCV.get_feature_names_out())\n",
    "# data_final = pd.concat([data_final,df_nlp2[\"first_party_winner\"]],axis=1,join='inner')\n",
    "# terms = vectorizeCV.get_feature_names_out()\n",
    "# data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseVec(vec,nlp):\n",
    "    \"\"\"\n",
    "        벡터 고르는 함수입니다.\n",
    "        vec 파라미터로 tf-idf이면 'tf', count-vector는 'cv'.\n",
    "        nlp는 깨끗하게 전처리된 데이터셋을 보내주세요.\n",
    "    \"\"\"\n",
    "    global terms, vectorize\n",
    "    testSet = False\n",
    "\n",
    "    if nlp.equals(dfTest):\n",
    "        count_matrix = vectorize.transform(nlp['facts_clean'])\n",
    "        testSet = True\n",
    "    \n",
    "    if vec == 'tf' and testSet == False:\n",
    "        #TFIDF / 전체 문서들 중에서 빈도수가 높으면 가중치가 낮고, 특정 문서에서 빈도수가 높으면 가중치를 높게 줌\n",
    "        vectorize = TfidfVectorizer()\n",
    "        count_matrix = vectorize.fit_transform(nlp['facts_clean'])\n",
    "\n",
    "    if vec == 'cv'and testSet == False:\n",
    "        # 카운터 벡터라이즈 / 전체 문서에서 빈도수가 높으면 가중치가 높음\n",
    "        vectorize=CountVectorizer()\n",
    "        count_matrix = vectorize.fit_transform(nlp['facts_clean'])\n",
    "\n",
    "    count_array = count_matrix.toarray()\n",
    "    data_final = pd.DataFrame(data=count_array,columns = vectorize.get_feature_names_out())\n",
    "    data_final = pd.concat([data_final,nlp[\"first_party_winner\"]],axis=1,join='inner')\n",
    "    terms = vectorize.get_feature_names_out()\n",
    "    testSet = False\n",
    "    return data_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = chooseVec('tf', df_nlp2) # td-idf : 'tf' / counter vector : 'cv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(data_final.drop(columns=['first_party_winner']), data_final['first_party_winner'], test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# lda = LatentDirichletAllocation(n_components=200, random_state=1) # n_component : 토픽 갯수\n",
    "# pd.DataFrame(lda.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 쪼개기\n",
    "def splitData(data):\n",
    "    global X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['first_party_winner']), data['first_party_winner'], test_size=0.3,random_state=0)\n",
    "\n",
    "# LDA 학습\n",
    "def ldaTraining(data, n):\n",
    "    \"\"\"\n",
    "        X_trian과 lDA 적용 후 확인할 데이터 n개\n",
    "    \"\"\"\n",
    "    # LDA 학습, 단어의 의미구조 파악\n",
    "    # 30초정도 걸림\n",
    "    global lda, X_train\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    lda = LatentDirichletAllocation(n_components=200, random_state=1) # n_component : 토픽 갯수\n",
    "    lda_data = lda.fit_transform(data)\n",
    "    X_train = pd.DataFrame(data=lda_data)\n",
    "    # LDA 학습 결과 보기\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        print(\"Topic %d:\" % (idx+1), [(terms[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "\n",
    "# 모델 만들기\n",
    "def modeling(params):\n",
    "    \"\"\"\n",
    "        XGB 학습 모델 함수.\n",
    "        XGB 파라미터를 받습니다.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    model = XGBClassifier(**params) # **param_dict\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# 예측하기\n",
    "def prediction(data):\n",
    "    \"\"\"\n",
    "        lda 적용된 테스트 데이터 예측하기\n",
    "    \"\"\"\n",
    "    lda_data_t = pd.DataFrame(data=lda.transform(data)) # 테스트 lda 적용\n",
    "    y_pred = model.predict(lda_data_t)\n",
    "    y_pred_df = pd.DataFrame(data=y_pred, columns=['data'])\n",
    "    display(y_pred_df.value_counts())\n",
    "\n",
    "    # 평가\n",
    "    from sklearn.metrics import f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred) # 정확도\n",
    "    print(\"Accuracy : %.2f%%\" % (accuracy * 100.0)) \n",
    "    print('f1_score :', f1_score(y_test, y_pred)) # 정밀도와 재현율의 조화평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('juvenile', 2.03), ('minor', 1.86), ('island', 1.56), ('nixon', 1.51), ('rhode', 1.02)]\n",
      "Topic 2: [('morrison', 1.54), ('ford', 0.95), ('accommodation', 0.77), ('title', 0.66), ('tssaa', 0.61)]\n",
      "Topic 3: [('minnesota', 0.91), ('fuel', 0.71), ('energy', 0.66), ('savchuk', 0.58), ('rush', 0.55)]\n",
      "Topic 4: [('coal', 0.74), ('mapp', 0.62), ('hernandez', 0.6), ('hayden', 0.59), ('ursery', 0.46)]\n",
      "Topic 5: [('reform', 1.05), ('created', 0.85), ('finance', 0.81), ('rhine', 0.8), ('element', 0.69)]\n",
      "Topic 6: [('granite', 0.61), ('zicherman', 0.58), ('consumer', 0.55), ('decedent', 0.55), ('minton', 0.53)]\n",
      "Topic 7: [('distress', 0.96), ('emotional', 0.93), ('intentional', 0.87), ('college', 0.55), ('franchisees', 0.54)]\n",
      "Topic 8: [('limitation', 2.9), ('run', 1.47), ('tribe', 0.99), ('treaty', 0.97), ('casino', 0.87)]\n",
      "Topic 9: [('penalty', 5.85), ('payment', 2.32), ('irs', 2.19), ('crime', 2.12), ('plea', 2.11)]\n",
      "Topic 10: [('hubbell', 0.57), ('florence', 0.55), ('robers', 0.43), ('restitution', 0.33), ('hubbells', 0.29)]\n",
      "Topic 11: [('vermont', 0.61), ('wyeth', 0.51), ('trailer', 0.5), ('banks', 0.42), ('levine', 0.41)]\n",
      "Topic 12: [('preclearance', 1.13), ('unavailable', 0.95), ('tyler', 0.65), ('cantrell', 0.61), ('change', 0.53)]\n",
      "Topic 13: [('alabama', 1.86), ('compelling', 1.03), ('proposition', 1.01), ('strict', 0.85), ('scrutiny', 0.82)]\n",
      "Topic 14: [('rico', 2.75), ('abortion', 2.26), ('status', 1.59), ('doctor', 1.14), ('racketeer', 1.12)]\n",
      "Topic 15: [('fcc', 3.68), ('communication', 2.23), ('telecommunication', 2.06), ('network', 1.35), ('telephone', 1.32)]\n",
      "Topic 16: [('carolina', 3.33), ('north', 2.41), ('delay', 1.83), ('signed', 1.6), ('waiver', 1.32)]\n",
      "Topic 17: [('lot', 0.77), ('landrigan', 0.6), ('appear', 0.59), ('leaving', 0.55), ('underwriter', 0.52)]\n",
      "Topic 18: [('gary', 0.92), ('tenure', 0.72), ('professor', 0.58), ('ricky', 0.57), ('village', 0.5)]\n",
      "Topic 19: [('guam', 0.66), ('gideon', 0.65), ('adelaide', 0.6), ('humana', 0.51), ('watkinson', 0.47)]\n",
      "Topic 20: [('debtor', 1.04), ('interrogation', 0.52), ('alliance', 0.46), ('questionnaire', 0.44), ('repayment', 0.44)]\n",
      "Topic 21: [('jimenez', 0.84), ('knight', 0.66), ('staff', 0.6), ('gerrymandering', 0.44), ('consent', 0.44)]\n",
      "Topic 22: [('film', 0.95), ('atkins', 0.79), ('targeted', 0.73), ('bfp', 0.72), ('berenyi', 0.69)]\n",
      "Topic 23: [('lange', 0.89), ('pea', 0.45), ('ssa', 0.42), ('art', 0.41), ('nea', 0.37)]\n",
      "Topic 24: [('maine', 2.0), ('selective', 0.56), ('maclean', 0.54), ('devlin', 0.46), ('wayte', 0.42)]\n",
      "Topic 25: [('moore', 0.64), ('sundiamond', 0.62), ('pereida', 0.61), ('cancer', 0.59), ('hall', 0.57)]\n",
      "Topic 26: [('rluipa', 0.75), ('vopa', 0.67), ('acre', 0.67), ('doe', 0.63), ('chapel', 0.58)]\n",
      "Topic 27: [('saturday', 0.67), ('sunday', 0.61), ('lozman', 0.57), ('background', 0.52), ('sabbath', 0.51)]\n",
      "Topic 28: [('dea', 0.89), ('liberty', 0.79), ('horowitz', 0.7), ('dirk', 0.69), ('anderson', 0.68)]\n",
      "Topic 29: [('prisoner', 1.87), ('inmate', 1.74), ('church', 1.38), ('correction', 1.03), ('chesternut', 0.56)]\n",
      "Topic 30: [('child', 1.18), ('pornography', 1.15), ('unconstitutionally', 0.96), ('protect', 0.89), ('overbroad', 0.78)]\n",
      "Topic 31: [('connelly', 0.74), ('castro', 0.66), ('syndicalism', 0.43), ('write', 0.4), ('klan', 0.39)]\n",
      "Topic 32: [('arbitration', 7.19), ('agreement', 2.08), ('compel', 1.5), ('arbitrator', 1.27), ('stay', 1.24)]\n",
      "Topic 33: [('dependent', 0.71), ('eo', 0.59), ('wachovia', 0.46), ('region', 0.45), ('ncrl', 0.42)]\n",
      "Topic 34: [('search', 6.59), ('car', 5.4), ('officer', 5.39), ('police', 5.36), ('vehicle', 4.25)]\n",
      "Topic 35: [('fdcpa', 0.86), ('vendor', 0.84), ('morton', 0.71), ('patane', 0.59), ('brathwaite', 0.58)]\n",
      "Topic 36: [('air', 0.79), ('musacchio', 0.58), ('mitchell', 0.53), ('ets', 0.5), ('bush', 0.49)]\n",
      "Topic 37: [('leaf', 0.7), ('pregnant', 0.69), ('pregnancy', 0.66), ('patchak', 0.62), ('enmund', 0.61)]\n",
      "Topic 38: [('establishment', 1.58), ('perkins', 1.0), ('hana', 0.8), ('byrd', 0.63), ('sergeant', 0.6)]\n",
      "Topic 39: [('nevada', 1.09), ('ake', 0.89), ('psychiatrist', 0.88), ('partnership', 0.74), ('kyles', 0.67)]\n",
      "Topic 40: [('pereira', 0.89), ('hall', 0.8), ('rock', 0.75), ('pertinent', 0.59), ('mickens', 0.54)]\n",
      "Topic 41: [('mansell', 0.89), ('lancaster', 0.73), ('primus', 0.72), ('breuer', 0.49), ('major', 0.46)]\n",
      "Topic 42: [('town', 0.65), ('pipeline', 0.62), ('louisiana', 0.54), ('tobacco', 0.49), ('barbier', 0.46)]\n",
      "Topic 43: [('costa', 0.63), ('bspa', 0.63), ('liu', 0.58), ('krupski', 0.48), ('beach', 0.48)]\n",
      "Topic 44: [('lane', 0.6), ('hepps', 0.57), ('hen', 0.53), ('leath', 0.51), ('bumper', 0.51)]\n",
      "Topic 45: [('tolling', 0.93), ('equitable', 0.89), ('clay', 0.68), ('lsd', 0.56), ('oconnor', 0.55)]\n",
      "Topic 46: [('propulsion', 0.47), ('flood', 0.47), ('canal', 0.46), ('borden', 0.41), ('transamerica', 0.39)]\n",
      "Topic 47: [('burnham', 0.68), ('coolidge', 0.59), ('ibm', 0.55), ('barefoot', 0.55), ('carpenter', 0.44)]\n",
      "Topic 48: [('facto', 1.16), ('post', 1.15), ('ex', 1.11), ('ifc', 0.68), ('aguilar', 0.59)]\n",
      "Topic 49: [('tax', 7.85), ('income', 4.06), ('retirement', 4.03), ('refund', 3.28), ('paid', 3.15)]\n",
      "Topic 50: [('deck', 0.99), ('peguero', 0.69), ('anderson', 0.68), ('romag', 0.6), ('fossil', 0.52)]\n",
      "Topic 51: [('discriminatory', 1.23), ('racially', 0.87), ('rosemond', 0.74), ('summer', 0.63), ('ogilvies', 0.58)]\n",
      "Topic 52: [('amy', 0.61), ('thornton', 0.53), ('traveling', 0.52), ('rivet', 0.5), ('luggage', 0.45)]\n",
      "Topic 53: [('aedpa', 1.67), ('antiterrorism', 1.34), ('penalty', 0.9), ('row', 0.77), ('madison', 0.74)]\n",
      "Topic 54: [('orlando', 0.66), ('ivd', 0.53), ('venturas', 0.44), ('support', 0.44), ('atherton', 0.4)]\n",
      "Topic 55: [('sign', 0.62), ('obvious', 0.57), ('ksr', 0.55), ('farm', 0.48), ('campbell', 0.48)]\n",
      "Topic 56: [('barker', 1.03), ('marathon', 0.86), ('roberson', 0.72), ('description', 0.68), ('james', 0.66)]\n",
      "Topic 57: [('hampshire', 0.9), ('stanton', 0.78), ('holland', 0.6), ('coker', 0.55), ('piper', 0.5)]\n",
      "Topic 58: [('va', 0.64), ('lewis', 0.62), ('percentage', 0.57), ('minority', 0.49), ('mille', 0.48)]\n",
      "Topic 59: [('davis', 1.77), ('article', 1.46), ('iii', 1.24), ('puerto', 1.07), ('batson', 0.81)]\n",
      "Topic 60: [('districting', 0.76), ('aaron', 0.66), ('governor', 0.58), ('chattanooga', 0.54), ('slate', 0.46)]\n",
      "Topic 61: [('reef', 1.08), ('pain', 1.07), ('dakota', 1.03), ('warn', 0.91), ('daily', 0.82)]\n",
      "Topic 62: [('jeopardy', 3.51), ('double', 3.12), ('bank', 0.79), ('sierra', 0.63), ('club', 0.56)]\n",
      "Topic 63: [('scott', 1.03), ('lawson', 0.62), ('till', 0.45), ('rehberg', 0.41), ('hodges', 0.41)]\n",
      "Topic 64: [('romeo', 0.78), ('cobb', 0.7), ('confessed', 0.54), ('detroit', 0.52), ('remedial', 0.4)]\n",
      "Topic 65: [('fletcher', 0.62), ('wilson', 0.51), ('bracy', 0.51), ('edward', 0.49), ('noel', 0.47)]\n",
      "Topic 66: [('nation', 0.67), ('legislation', 0.67), ('gypsum', 0.62), ('amgen', 0.55), ('dans', 0.53)]\n",
      "Topic 67: [('blood', 0.97), ('doe', 0.68), ('apjs', 0.6), ('herdman', 0.58), ('epcra', 0.57)]\n",
      "Topic 68: [('supervised', 1.43), ('copyright', 0.95), ('moran', 0.76), ('holguin', 0.76), ('kisela', 0.68)]\n",
      "Topic 69: [('idea', 1.33), ('patient', 0.87), ('questioned', 0.67), ('ferbar', 0.59), ('eagan', 0.56)]\n",
      "Topic 70: [('airline', 0.99), ('bradley', 0.95), ('warren', 0.92), ('northwest', 0.88), ('madden', 0.75)]\n",
      "Topic 71: [('mimms', 0.67), ('jcg', 0.48), ('jcm', 0.48), ('moore', 0.45), ('nrab', 0.42)]\n",
      "Topic 72: [('kaleys', 0.76), ('belmontes', 0.76), ('chafin', 0.57), ('glenn', 0.56), ('icc', 0.56)]\n",
      "Topic 73: [('johnson', 0.64), ('iraq', 0.6), ('button', 0.53), ('debtor', 0.43), ('valuable', 0.42)]\n",
      "Topic 74: [('mission', 0.75), ('dai', 0.58), ('contractor', 0.55), ('project', 0.53), ('limit', 0.51)]\n",
      "Topic 75: [('taylor', 0.9), ('flag', 0.78), ('boynton', 0.61), ('welsh', 0.61), ('caiii', 0.54)]\n",
      "Topic 76: [('gray', 0.77), ('rogers', 0.63), ('certification', 0.59), ('threat', 0.45), ('brown', 0.44)]\n",
      "Topic 77: [('team', 0.66), ('ohralik', 0.64), ('laidlaw', 0.63), ('nestor', 0.55), ('baer', 0.43)]\n",
      "Topic 78: [('nlra', 0.78), ('young', 0.74), ('risk', 0.65), ('age', 0.58), ('treasure', 0.55)]\n",
      "Topic 79: [('hughes', 0.87), ('lopez', 0.66), ('adam', 0.59), ('kent', 0.59), ('knetsch', 0.57)]\n",
      "Topic 80: [('tenant', 0.7), ('doggett', 0.68), ('cline', 0.51), ('ramirezs', 0.41), ('buffer', 0.4)]\n",
      "Topic 81: [('muniz', 0.73), ('nguyen', 0.6), ('capacity', 0.54), ('amaa', 0.48), ('psychiatric', 0.44)]\n",
      "Topic 82: [('basic', 0.8), ('boycott', 0.75), ('wage', 0.7), ('matlock', 0.65), ('nautilus', 0.57)]\n",
      "Topic 83: [('high', 1.17), ('fined', 0.98), ('suspension', 0.88), ('estate', 0.83), ('tucker', 0.73)]\n",
      "Topic 84: [('wilkinson', 0.59), ('subcommittee', 0.52), ('regent', 0.46), ('kqed', 0.46), ('gilmore', 0.44)]\n",
      "Topic 85: [('los', 1.86), ('angeles', 1.82), ('advisor', 0.91), ('congressional', 0.66), ('columbia', 0.64)]\n",
      "Topic 86: [('hick', 0.79), ('tribal', 0.61), ('female', 0.59), ('cca', 0.49), ('curtis', 0.48)]\n",
      "Topic 87: [('standing', 3.06), ('covered', 2.44), ('letter', 2.35), ('candidate', 1.94), ('sent', 1.81)]\n",
      "Topic 88: [('foster', 0.77), ('photo', 0.7), ('ash', 0.68), ('nassar', 0.44), ('utsw', 0.44)]\n",
      "Topic 89: [('arkansas', 1.03), ('december', 0.82), ('nlrb', 0.73), ('brumfield', 0.63), ('clemency', 0.6)]\n",
      "Topic 90: [('fca', 0.67), ('brada', 0.56), ('ayestas', 0.55), ('hughes', 0.54), ('dune', 0.52)]\n",
      "Topic 91: [('broadcasting', 0.83), ('maslenjak', 0.66), ('broadcast', 0.58), ('cocaine', 0.53), ('stoddard', 0.52)]\n",
      "Topic 92: [('judge', 13.52), ('death', 13.3), ('murder', 12.67), ('habeas', 12.62), ('counsel', 11.85)]\n",
      "Topic 93: [('bargaining', 2.22), ('collective', 1.82), ('designated', 0.98), ('faa', 0.93), ('retire', 0.75)]\n",
      "Topic 94: [('iowa', 1.83), ('disciplinary', 0.87), ('charge', 0.76), ('marquez', 0.7), ('fsa', 0.69)]\n",
      "Topic 95: [('milk', 0.78), ('exxon', 0.66), ('purchaser', 0.64), ('mccutchen', 0.61), ('air', 0.61)]\n",
      "Topic 96: [('misdemeanor', 1.7), ('domestic', 1.44), ('fish', 1.08), ('violence', 0.78), ('egelhoff', 0.72)]\n",
      "Topic 97: [('lyon', 1.14), ('fikes', 0.71), ('fisher', 0.71), ('sacketts', 0.67), ('senate', 0.67)]\n",
      "Topic 98: [('abramski', 0.69), ('ncaa', 0.57), ('byars', 0.53), ('belton', 0.53), ('shuttlesworth', 0.47)]\n",
      "Topic 99: [('moncrieffe', 0.76), ('faretta', 0.75), ('mathis', 0.63), ('homeland', 0.51), ('daca', 0.46)]\n",
      "Topic 100: [('hudson', 1.1), ('omega', 0.63), ('carlisle', 0.61), ('morrissey', 0.59), ('hawaii', 0.49)]\n",
      "Topic 101: [('deputy', 1.13), ('inspector', 0.82), ('campbell', 0.76), ('weapon', 0.76), ('rodgers', 0.72)]\n",
      "Topic 102: [('statute', 10.07), ('national', 6.69), ('damage', 6.1), ('secretary', 5.6), ('use', 5.1)]\n",
      "Topic 103: [('mcdonald', 1.01), ('gonzales', 0.99), ('archer', 0.81), ('battery', 0.76), ('cicenia', 0.74)]\n",
      "Topic 104: [('patent', 7.86), ('infringement', 2.87), ('perry', 1.42), ('product', 1.2), ('sale', 0.95)]\n",
      "Topic 105: [('busing', 0.76), ('spink', 0.71), ('mitchell', 0.66), ('schad', 0.57), ('manslaughter', 0.52)]\n",
      "Topic 106: [('richmond', 0.75), ('atlantic', 0.71), ('fox', 0.7), ('crew', 0.7), ('kirtsaeng', 0.67)]\n",
      "Topic 107: [('sealy', 0.86), ('expenditure', 0.56), ('hardship', 0.51), ('undue', 0.47), ('airway', 0.39)]\n",
      "Topic 108: [('san', 0.82), ('antonio', 0.78), ('samta', 0.53), ('camp', 0.49), ('intrastate', 0.47)]\n",
      "Topic 109: [('professional', 0.55), ('arco', 0.52), ('mckinneys', 0.49), ('severance', 0.47), ('liquor', 0.42)]\n",
      "Topic 110: [('indebtedness', 0.78), ('farm', 0.67), ('donald', 0.59), ('cooper', 0.51), ('molitoris', 0.51)]\n",
      "Topic 111: [('cuba', 0.82), ('overthrow', 0.64), ('passport', 0.63), ('connecticut', 0.59), ('sugar', 0.56)]\n",
      "Topic 112: [('mill', 0.83), ('timbs', 0.68), ('colorado', 0.65), ('pollard', 0.62), ('cftb', 0.62)]\n",
      "Topic 113: [('buckhannon', 0.66), ('burma', 0.66), ('ideal', 0.52), ('westerngeco', 0.47), ('ion', 0.47)]\n",
      "Topic 114: [('halbert', 0.71), ('richards', 0.68), ('sca', 0.63), ('top', 0.57), ('approach', 0.57)]\n",
      "Topic 115: [('school', 18.21), ('reversed', 11.91), ('service', 11.05), ('policy', 10.33), ('attorney', 9.78)]\n",
      "Topic 116: [('crooker', 0.73), ('michigan', 0.62), ('gonzalez', 0.47), ('ancsa', 0.46), ('restitution', 0.44)]\n",
      "Topic 117: [('police', 7.29), ('officer', 5.98), ('confession', 4.88), ('virginia', 4.36), ('arrested', 4.3)]\n",
      "Topic 118: [('hobbs', 0.86), ('sims', 0.74), ('riley', 0.71), ('extortion', 0.69), ('gang', 0.68)]\n",
      "Topic 119: [('deference', 1.17), ('passenger', 0.77), ('hillery', 0.71), ('bus', 0.67), ('chevron', 0.55)]\n",
      "Topic 120: [('cooley', 0.69), ('idaho', 0.63), ('vl', 0.59), ('el', 0.48), ('nonnative', 0.48)]\n",
      "Topic 121: [('proposed', 1.87), ('hotel', 1.06), ('continuing', 1.03), ('compliance', 0.95), ('unanimously', 0.94)]\n",
      "Topic 122: [('citizenship', 1.37), ('father', 1.31), ('hartford', 1.09), ('supplemental', 0.92), ('ford', 0.8)]\n",
      "Topic 123: [('rosenbloom', 0.65), ('katz', 0.59), ('castillo', 0.58), ('easement', 0.5), ('railway', 0.48)]\n",
      "Topic 124: [('career', 1.23), ('serious', 0.76), ('village', 0.65), ('candidate', 0.64), ('tiebreaker', 0.58)]\n",
      "Topic 125: [('electricity', 0.68), ('utility', 0.65), ('google', 0.54), ('knudson', 0.54), ('loudermill', 0.45)]\n",
      "Topic 126: [('court', 86.93), ('state', 45.0), ('district', 44.57), ('federal', 35.06), ('appeal', 33.67)]\n",
      "Topic 127: [('girl', 0.79), ('segregation', 0.59), ('torture', 0.56), ('palestinian', 0.45), ('adderley', 0.44)]\n",
      "Topic 128: [('montgomery', 1.2), ('beaudreaux', 0.72), ('isaac', 0.6), ('fraud', 0.54), ('cable', 0.47)]\n",
      "Topic 129: [('spouse', 1.08), ('pension', 0.8), ('heck', 0.63), ('wilson', 0.59), ('ressam', 0.56)]\n",
      "Topic 130: [('unknown', 0.89), ('confederate', 0.52), ('fuller', 0.51), ('committing', 0.49), ('hernández', 0.4)]\n",
      "Topic 131: [('male', 0.75), ('pepper', 0.67), ('restricted', 0.66), ('chiarella', 0.62), ('target', 0.56)]\n",
      "Topic 132: [('splash', 0.64), ('rotary', 0.54), ('hutcheson', 0.5), ('menon', 0.43), ('isda', 0.38)]\n",
      "Topic 133: [('cable', 1.7), ('channel', 1.24), ('television', 0.96), ('hardwick', 0.72), ('competition', 0.7)]\n",
      "Topic 134: [('woodson', 0.97), ('gregg', 0.96), ('jurek', 0.77), ('proffitt', 0.77), ('powell', 0.74)]\n",
      "Topic 135: [('ballot', 2.17), ('vote', 1.76), ('candidate', 1.31), ('presidential', 1.22), ('elector', 1.07)]\n",
      "Topic 136: [('bankruptcy', 11.1), ('tax', 7.5), ('debt', 3.69), ('chapter', 3.69), ('code', 3.19)]\n",
      "Topic 137: [('fox', 0.79), ('ashcroft', 0.61), ('csa', 0.49), ('athlete', 0.47), ('frescati', 0.47)]\n",
      "Topic 138: [('amgen', 0.65), ('plumer', 0.64), ('phoenix', 0.59), ('bridge', 0.56), ('colorado', 0.48)]\n",
      "Topic 139: [('afroyim', 0.73), ('cia', 0.59), ('amant', 0.54), ('crawford', 0.51), ('puerto', 0.47)]\n",
      "Topic 140: [('solid', 1.16), ('importation', 0.79), ('waste', 0.67), ('liquid', 0.62), ('vornado', 0.6)]\n",
      "Topic 141: [('airport', 0.67), ('fica', 0.66), ('florida', 0.53), ('microsoft', 0.53), ('attainder', 0.53)]\n",
      "Topic 142: [('pm', 0.6), ('ncaa', 0.57), ('ssdi', 0.54), ('franklin', 0.54), ('orden', 0.51)]\n",
      "Topic 143: [('spano', 0.75), ('greer', 0.68), ('escobedo', 0.68), ('atwater', 0.58), ('massachusetts', 0.55)]\n",
      "Topic 144: [('naacp', 1.74), ('johnson', 1.35), ('alabama', 0.94), ('nurse', 0.78), ('dot', 0.76)]\n",
      "Topic 145: [('fisher', 0.58), ('inference', 0.55), ('girlfriend', 0.54), ('afla', 0.53), ('parker', 0.45)]\n",
      "Topic 146: [('young', 0.49), ('mining', 0.39), ('sabri', 0.29), ('dispersed', 0.29), ('bribe', 0.24)]\n",
      "Topic 147: [('postal', 0.68), ('hillman', 0.6), ('gregory', 0.56), ('thompson', 0.54), ('attachment', 0.53)]\n",
      "Topic 148: [('dna', 0.69), ('meyers', 0.65), ('dawson', 0.63), ('redding', 0.61), ('drug', 0.59)]\n",
      "Topic 149: [('bg', 0.59), ('bop', 0.51), ('cooper', 0.47), ('argentina', 0.47), ('lopez', 0.44)]\n",
      "Topic 150: [('appointment', 1.27), ('dc', 1.23), ('defender', 1.11), ('alj', 0.94), ('deficient', 0.74)]\n",
      "Topic 151: [('limit', 1.22), ('committee', 0.85), ('tva', 0.61), ('sprint', 0.54), ('political', 0.47)]\n",
      "Topic 152: [('age', 0.93), ('discon', 0.61), ('crawford', 0.61), ('samuel', 0.56), ('red', 0.51)]\n",
      "Topic 153: [('available', 1.71), ('solicitation', 0.96), ('exhaust', 0.86), ('remedy', 0.81), ('offset', 0.77)]\n",
      "Topic 154: [('rawlinson', 0.69), ('arbitrability', 0.39), ('branch', 0.34), ('schein', 0.25), ('scab', 0.23)]\n",
      "Topic 155: [('deportable', 0.88), ('slack', 0.88), ('lien', 0.86), ('drug', 0.82), ('lawyer', 0.78)]\n",
      "Topic 156: [('grazing', 0.78), ('laundering', 0.66), ('cooperative', 0.63), ('taylor', 0.55), ('herrick', 0.53)]\n",
      "Topic 157: [('michael', 0.67), ('rickard', 0.59), ('nelson', 0.57), ('akamai', 0.53), ('leu', 0.53)]\n",
      "Topic 158: [('henry', 0.83), ('chinese', 0.83), ('visciotti', 0.72), ('mandate', 0.68), ('user', 0.68)]\n",
      "Topic 159: [('allen', 0.81), ('severance', 0.77), ('traveler', 0.67), ('mullenix', 0.67), ('oklahoma', 0.65)]\n",
      "Topic 160: [('jail', 1.08), ('answer', 1.02), ('fine', 0.89), ('huff', 0.72), ('brown', 0.61)]\n",
      "Topic 161: [('disability', 3.12), ('ada', 2.74), ('transportation', 2.0), ('purchase', 1.96), ('chemical', 1.87)]\n",
      "Topic 162: [('saving', 1.2), ('value', 0.96), ('subdivision', 0.86), ('tax', 0.86), ('deficit', 0.64)]\n",
      "Topic 163: [('convention', 1.09), ('kent', 0.74), ('yang', 0.71), ('setser', 0.7), ('overmyer', 0.67)]\n",
      "Topic 164: [('muhammad', 0.71), ('gm', 0.68), ('chain', 0.56), ('protocol', 0.44), ('grocery', 0.42)]\n",
      "Topic 165: [('earlier', 1.37), ('warning', 1.04), ('forest', 0.97), ('claimant', 0.87), ('correct', 0.86)]\n",
      "Topic 166: [('burbine', 0.71), ('whayne', 0.61), ('daca', 0.59), ('mcfl', 0.57), ('posing', 0.54)]\n",
      "Topic 167: [('ad', 0.61), ('expressly', 0.57), ('sekhar', 0.5), ('wearrys', 0.46), ('wearry', 0.46)]\n",
      "Topic 168: [('execution', 1.72), ('atkins', 1.48), ('kentucky', 1.24), ('th', 0.99), ('advice', 0.99)]\n",
      "Topic 169: [('technology', 0.56), ('buchanan', 0.56), ('lorenzo', 0.49), ('davis', 0.45), ('adverse', 0.43)]\n",
      "Topic 170: [('townsend', 1.09), ('alford', 0.73), ('charlottemecklenburg', 0.69), ('cole', 0.66), ('balisok', 0.62)]\n",
      "Topic 171: [('rippo', 0.7), ('yates', 0.64), ('tobacco', 0.5), ('abatement', 0.46), ('shakedown', 0.43)]\n",
      "Topic 172: [('jackson', 0.67), ('socket', 0.66), ('alex', 0.57), ('zablocki', 0.51), ('redhail', 0.51)]\n",
      "Topic 173: [('creation', 0.82), ('manufacture', 0.81), ('gme', 0.59), ('stewart', 0.57), ('incorporated', 0.57)]\n",
      "Topic 174: [('innis', 0.74), ('boyle', 0.65), ('negusie', 0.6), ('apple', 0.6), ('partnership', 0.52)]\n",
      "Topic 175: [('gmd', 0.6), ('arizona', 0.45), ('mathematical', 0.43), ('alliance', 0.42), ('examiner', 0.4)]\n",
      "Topic 176: [('waste', 0.83), ('crown', 0.8), ('motorcycle', 0.64), ('currier', 0.6), ('hargis', 0.58)]\n",
      "Topic 177: [('martinez', 1.29), ('advertising', 0.65), ('suders', 0.63), ('kasten', 0.42), ('casey', 0.4)]\n",
      "Topic 178: [('watkins', 0.73), ('device', 0.73), ('home', 0.59), ('nike', 0.57), ('citibank', 0.54)]\n",
      "Topic 179: [('specie', 1.57), ('endangered', 1.22), ('esa', 0.98), ('wildlife', 0.8), ('fws', 0.65)]\n",
      "Topic 180: [('civilian', 0.81), ('chavis', 0.78), ('guard', 0.65), ('charter', 0.64), ('coast', 0.6)]\n",
      "Topic 181: [('religious', 0.66), ('religion', 0.5), ('giglios', 0.47), ('giglio', 0.45), ('hoasca', 0.43)]\n",
      "Topic 182: [('unit', 0.82), ('habitat', 0.72), ('created', 0.71), ('maintain', 0.7), ('african', 0.68)]\n",
      "Topic 183: [('salina', 0.86), ('nevada', 0.67), ('yacht', 0.63), ('sisson', 0.53), ('obduskey', 0.47)]\n",
      "Topic 184: [('mitt', 0.67), ('field', 0.66), ('stun', 0.59), ('marshall', 0.56), ('westside', 0.54)]\n",
      "Topic 185: [('parking', 0.54), ('agard', 0.52), ('shaw', 0.45), ('heir', 0.4), ('agards', 0.39)]\n",
      "Topic 186: [('wood', 1.15), ('democrat', 1.01), ('democratic', 0.59), ('picketing', 0.56), ('insider', 0.47)]\n",
      "Topic 187: [('government', 8.94), ('convicted', 8.42), ('rule', 8.14), ('guilty', 7.9), ('conviction', 7.9)]\n",
      "Topic 188: [('unusual', 1.07), ('cruel', 1.0), ('punishment', 0.74), ('booth', 0.7), ('suisse', 0.65)]\n",
      "Topic 189: [('design', 0.81), ('robbins', 0.74), ('willfully', 0.64), ('hazen', 0.58), ('mdi', 0.56)]\n",
      "Topic 190: [('reliance', 0.79), ('wharf', 0.74), ('husain', 0.57), ('hardt', 0.52), ('hanson', 0.43)]\n",
      "Topic 191: [('illegally', 1.09), ('read', 0.75), ('metropolitan', 0.73), ('army', 0.72), ('modify', 0.66)]\n",
      "Topic 192: [('gamble', 0.82), ('medellin', 0.71), ('clair', 0.63), ('nichols', 0.63), ('barton', 0.55)]\n",
      "Topic 193: [('title', 3.72), ('banc', 3.03), ('en', 3.01), ('vii', 2.67), ('eeoc', 1.83)]\n",
      "Topic 194: [('juror', 2.89), ('commonwealth', 0.72), ('operate', 0.71), ('rompillas', 0.7), ('wiggins', 0.68)]\n",
      "Topic 195: [('university', 3.86), ('admission', 3.02), ('white', 2.93), ('victim', 2.76), ('challenge', 2.49)]\n",
      "Topic 196: [('immigration', 7.38), ('alien', 2.16), ('ina', 2.11), ('deportation', 2.09), ('removal', 2.01)]\n",
      "Topic 197: [('peel', 0.6), ('maxwelljolly', 0.55), ('watson', 0.54), ('cutback', 0.41), ('nbta', 0.3)]\n",
      "Topic 198: [('prisoner', 0.92), ('omission', 0.77), ('baker', 0.64), ('leonard', 0.58), ('drayton', 0.53)]\n",
      "Topic 199: [('flower', 0.77), ('hb', 0.53), ('prohibition', 0.5), ('length', 0.49), ('commitment', 0.45)]\n",
      "Topic 200: [('bond', 3.24), ('cause', 3.13), ('drug', 3.12), ('arrest', 2.87), ('began', 2.75)]\n"
     ]
    }
   ],
   "source": [
    "def fullTraining(data):\n",
    "    # 훈련\n",
    "    splitData(data)\n",
    "    ldaTraining(X_train, 5)\n",
    "\n",
    "    # XG부스트 모델 적용하기\n",
    "    default_gs_params = {'learning_rate': 0.9, 'max_depth': 9, 'scale_pos_weight': 0.3, 'subsample': 0.7}\n",
    "    modeling(default_gs_params)\n",
    "\n",
    "fullTraining(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data\n",
       "1       522\n",
       "0       222\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 54.97%\n",
      "f1_score : 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 테스트 적용\n",
    "prediction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.3, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.3, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.6, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.3s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=3, scale_pos_weight=0.3, subsample=0.9; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.7; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=5, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=7, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.5; total time=   0.1s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.7; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "[CV] END learning_rate=0.9, max_depth=9, scale_pos_weight=0.3, subsample=0.9; total time=   0.2s\n",
      "XGB 파라미터:  {'learning_rate': 0.9, 'max_depth': 9, 'scale_pos_weight': 0.3, 'subsample': 0.7}\n",
      "XGB 예측 정확도: 0.5882\n"
     ]
    }
   ],
   "source": [
    "# 그리드서치로 최적의 파라미터 값 찾아보기\n",
    "\n",
    "def findParam():\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    param_xgb = {\"scale_pos_weight\": [0.3],\n",
    "                 \"learning_rate\":[0.3, 0.6, 0.9], # 0~1 높을 수록 과적합 된다, 만약에 값이 낮으면 n_estimators를 높여야 과적합이 방지\n",
    "                 \"max_depth\":[3,5,7,9], # 보통 3~10, 높을 수록 과적합\n",
    "                 \"subsample\":[0.5, 0.7, 0.9] # 학습에 사용하는 샘플링 비율 0.5 ~ 1, 높을 수록 과적합\n",
    "                }    \n",
    "\n",
    "    gscv_xgb = GridSearchCV (estimator = model, param_grid = param_xgb, scoring ='accuracy', cv = 3, refit=True, n_jobs=1, verbose=2)\n",
    "    gscv_xgb.fit(X_train, y_train)\n",
    "    gs_params= gscv_xgb.best_params_\n",
    "    print('XGB 파라미터: ', gs_params)\n",
    "    print('XGB 예측 정확도: {:.4f}'.format(gscv_xgb.best_score_))\n",
    "\n",
    "    return gs_params\n",
    "# 그리드서치 후 파라미터 적용해서 모델링 학습 다시 하기\n",
    "# modeling(findParam())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트csv 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facts</th>\n",
       "      <th>facts_clean</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The  Bail Reform Act allowed the federal court...</td>\n",
       "      <td>bail reform act allowed federal court detain a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lexecon Inc. was a defendant in a class action...</td>\n",
       "      <td>lexecon inc defendant class action lawsuit usc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In  and , Fox Television Stations broadcast th...</td>\n",
       "      <td>fox television station broadcast billboard mus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During his trial for armed robbery of a federa...</td>\n",
       "      <td>trial armed robbery federally insured saving l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In , a magistrate judge issued a warrant autho...</td>\n",
       "      <td>magistrate judge issued warrant authorizing se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>According to Executive Order No.  signed by Pr...</td>\n",
       "      <td>according executive order signed president geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>Section (a) of the Clean Air Act (CAA) require...</td>\n",
       "      <td>section clean air act caa requires environment...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>Linda Matteo and John Madigan created a plan f...</td>\n",
       "      <td>linda matteo john madigan created plan utilizi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>In , the North Carolina Board of Agriculture a...</td>\n",
       "      <td>north carolina board agriculture adopted regul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>On August , , Dr. Paul Berheldt was stabbed to...</td>\n",
       "      <td>august dr paul berheldt stabbed death kitchen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  facts  \\\n",
       "0     The  Bail Reform Act allowed the federal court...   \n",
       "1     Lexecon Inc. was a defendant in a class action...   \n",
       "2     In  and , Fox Television Stations broadcast th...   \n",
       "3     During his trial for armed robbery of a federa...   \n",
       "4     In , a magistrate judge issued a warrant autho...   \n",
       "...                                                 ...   \n",
       "1235  According to Executive Order No.  signed by Pr...   \n",
       "1236  Section (a) of the Clean Air Act (CAA) require...   \n",
       "1237  Linda Matteo and John Madigan created a plan f...   \n",
       "1238  In , the North Carolina Board of Agriculture a...   \n",
       "1239  On August , , Dr. Paul Berheldt was stabbed to...   \n",
       "\n",
       "                                            facts_clean  first_party_winner  \n",
       "0     bail reform act allowed federal court detain a...                   0  \n",
       "1     lexecon inc defendant class action lawsuit usc...                   0  \n",
       "2     fox television station broadcast billboard mus...                   0  \n",
       "3     trial armed robbery federally insured saving l...                   0  \n",
       "4     magistrate judge issued warrant authorizing se...                   0  \n",
       "...                                                 ...                 ...  \n",
       "1235  according executive order signed president geo...                   0  \n",
       "1236  section clean air act caa requires environment...                   0  \n",
       "1237  linda matteo john madigan created plan utilizi...                   0  \n",
       "1238  north carolina board agriculture adopted regul...                   0  \n",
       "1239  august dr paul berheldt stabbed death kitchen ...                   0  \n",
       "\n",
       "[1240 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest['facts'] = dfTest['facts'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "dfTest['facts'] = dfTest['facts'].str.replace(r'\\d', '', regex=True)  # 숫자 제거\n",
    "dfTest[\"facts_clean\"] = dfTest[\"facts\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
    "dfTest['first_party_winner'] = np.zeros(len(dfTest)).astype(int)\n",
    "dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>aa</th>\n",
       "      <th>aacw</th>\n",
       "      <th>aai</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>...</th>\n",
       "      <th>zoneofinterests</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zt</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zurcher</th>\n",
       "      <th>zurchers</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurko</th>\n",
       "      <th>zurkos</th>\n",
       "      <th>first_party_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 12329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       __   aa  aacw  aai  aar  aaron   ab  abandon  abandoned  abandoning  \\\n",
       "0     0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1     0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "2     0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "3     0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "4     0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "...   ...  ...   ...  ...  ...    ...  ...      ...        ...         ...   \n",
       "1235  0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1236  0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1237  0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1238  0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1239  0.0  0.0   0.0  0.0  0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "\n",
       "      ...  zoneofinterests  zoning   zt  zuni  zurcher  zurchers  zurich  \\\n",
       "0     ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "1     ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "2     ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "3     ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "4     ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "...   ...              ...     ...  ...   ...      ...       ...     ...   \n",
       "1235  ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "1236  ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "1237  ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "1238  ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "1239  ...              0.0     0.0  0.0   0.0      0.0       0.0     0.0   \n",
       "\n",
       "      zurko  zurkos  first_party_winner  \n",
       "0       0.0     0.0                   0  \n",
       "1       0.0     0.0                   0  \n",
       "2       0.0     0.0                   0  \n",
       "3       0.0     0.0                   0  \n",
       "4       0.0     0.0                   0  \n",
       "...     ...     ...                 ...  \n",
       "1235    0.0     0.0                   0  \n",
       "1236    0.0     0.0                   0  \n",
       "1237    0.0     0.0                   0  \n",
       "1238    0.0     0.0                   0  \n",
       "1239    0.0     0.0                   0  \n",
       "\n",
       "[1240 rows x 12329 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터 고르기\n",
    "data_final_test = chooseVec('tf', dfTest)\n",
    "data_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPredict(dataTest):\n",
    "    global predcsv, X_test\n",
    "    \n",
    "    X_test = dataTest.drop(columns=['first_party_winner'])\n",
    "    y_test = dataTest['first_party_winner']\n",
    "    X_test = pd.DataFrame(data=lda.transform(X_test))\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    predcsv = pd.DataFrame(y_pred_test, columns=['first_party_winner'])\n",
    "\n",
    "testPredict(data_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission():\n",
    "    submit = pd.read_csv('C:/data/project/sample_submission.csv')\n",
    "    submit['first_party_winner'] = predcsv\n",
    "    submit.to_csv('./sample_submission.csv', index=False)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1                     926\n",
       "0                     314\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcsv.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셀프 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGoodSample():\n",
    "    # 데이터 준비\n",
    "    proba = model.predict_proba(X_test)\n",
    "    proba_df = pd.DataFrame(data=proba, columns=['proba_0','proba_1'])\n",
    "\n",
    "    # 0보다 1의 확률이 높은 데이터 추출\n",
    "    proba_higher = proba_df[proba_df['proba_1'] > proba_df['proba_0']]\n",
    "    # 1이 될 확률이 90프로 이상인 자료의 인덱스\n",
    "    proba_higher_index = proba_higher[proba_higher['proba_1'] > 0.9].index.to_list()\n",
    "\n",
    "    print('추가되는 샘플 갯수 :',len(proba_higher_index))\n",
    "    display(proba_higher_index)\n",
    "    # 인덱스 번호 리턴\n",
    "    return proba_higher_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatSample():\n",
    "    newIdx = findGoodSample()\n",
    "    filtered = dfTest.loc[dfTest.index.isin(newIdx)].drop(columns='facts')\n",
    "    filtered['first_party_winner'] = 1\n",
    "    new_df = pd.concat([df_nlp2,filtered]).reset_index(drop=True)\n",
    "    print('기존 샘플 갯수 :',len(df_nlp2))\n",
    "    print('새로운 프레임 데이터 갯수 :', len(new_df))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추가되는 샘플 갯수 : 644\n",
      "기존 샘플 갯수 : 2478\n",
      "새로운 프레임 데이터 갯수 : 3122\n",
      "Topic 1: [('cross', 0.65), ('posing', 0.65), ('coa', 0.63), ('hubbell', 0.57), ('nestor', 0.56)]\n",
      "Topic 2: [('bush', 1.24), ('townsend', 1.11), ('defaulted', 0.85), ('peguero', 0.7), ('partnership', 0.65)]\n",
      "Topic 3: [('dominguez', 1.23), ('plea', 0.91), ('evidentiary', 0.75), ('waller', 0.63), ('williams', 0.62)]\n",
      "Topic 4: [('abrams', 1.77), ('minority', 0.66), ('conference', 0.65), ('immigrant', 0.55), ('variance', 0.49)]\n",
      "Topic 5: [('generes', 1.37), ('trailer', 0.62), ('charles', 0.55), ('descamps', 0.52), ('bad', 0.49)]\n",
      "Topic 6: [('employee', 19.73), ('amendment', 18.72), ('board', 15.18), ('violation', 13.86), ('charge', 13.66)]\n",
      "Topic 7: [('merger', 0.76), ('rae', 0.68), ('egelhoff', 0.66), ('donna', 0.59), ('resendizponce', 0.56)]\n",
      "Topic 8: [('bond', 1.59), ('husband', 1.34), ('reef', 0.76), ('att', 0.68), ('internet', 0.67)]\n",
      "Topic 9: [('cocaine', 1.61), ('gram', 1.48), ('crack', 1.16), ('bradstreet', 1.1), ('greenmoss', 1.1)]\n",
      "Topic 10: [('substance', 2.55), ('controlled', 1.88), ('cpea', 0.99), ('salina', 0.81), ('romeo', 0.79)]\n",
      "Topic 11: [('english', 1.15), ('bladel', 1.0), ('jackson', 0.87), ('richards', 0.85), ('shoe', 0.85)]\n",
      "Topic 12: [('roth', 0.56), ('cic', 0.49), ('amber', 0.45), ('wong', 0.44), ('detention', 0.44)]\n",
      "Topic 13: [('gas', 2.12), ('force', 2.1), ('excessive', 1.73), ('immunity', 1.42), ('professor', 1.36)]\n",
      "Topic 14: [('maine', 1.51), ('massachusetts', 0.99), ('petrella', 0.55), ('corporate', 0.55), ('automatically', 0.54)]\n",
      "Topic 15: [('foster', 0.83), ('smith', 0.8), ('garner', 0.79), ('improper', 0.57), ('oncale', 0.55)]\n",
      "Topic 16: [('mandel', 0.73), ('amant', 0.55), ('huddleston', 0.51), ('stolen', 0.45), ('st', 0.43)]\n",
      "Topic 17: [('sullivan', 1.03), ('felt', 0.77), ('bullock', 0.74), ('morris', 0.7), ('epic', 0.67)]\n",
      "Topic 18: [('marshall', 1.04), ('estate', 0.9), ('apprendi', 0.87), ('minnick', 0.87), ('boynton', 0.62)]\n",
      "Topic 19: [('weber', 1.35), ('investment', 1.02), ('reserve', 0.99), ('dealer', 0.97), ('needed', 0.95)]\n",
      "Topic 20: [('energy', 0.8), ('radio', 0.7), ('cleanup', 0.67), ('jackpot', 0.67), ('contaminated', 0.52)]\n",
      "Topic 21: [('titlow', 0.8), ('defunis', 0.74), ('nichols', 0.63), ('lanza', 0.53), ('ramirez', 0.53)]\n",
      "Topic 22: [('hospital', 1.41), ('vra', 0.85), ('torres', 0.78), ('debt', 0.62), ('skinner', 0.6)]\n",
      "Topic 23: [('expenditure', 0.95), ('chemical', 0.89), ('haynes', 0.71), ('laidlaw', 0.64), ('clairs', 0.56)]\n",
      "Topic 24: [('abbott', 1.03), ('withholding', 0.64), ('chilean', 0.63), ('chile', 0.63), ('fpc', 0.59)]\n",
      "Topic 25: [('criminalhistory', 0.78), ('rosalesmireles', 0.78), ('score', 0.63), ('saratoga', 0.62), ('mccoy', 0.58)]\n",
      "Topic 26: [('steffel', 1.11), ('joshua', 0.81), ('flyer', 0.72), ('minnesota', 0.71), ('father', 0.67)]\n",
      "Topic 27: [('available', 1.7), ('osborne', 1.32), ('mclane', 0.74), ('exhausted', 0.66), ('mr', 0.62)]\n",
      "Topic 28: [('kelly', 2.29), ('brady', 2.06), ('agree', 1.06), ('castro', 0.96), ('grazing', 0.79)]\n",
      "Topic 29: [('fire', 1.23), ('price', 1.21), ('retail', 1.12), ('compelling', 1.01), ('wholesale', 0.93)]\n",
      "Topic 30: [('blood', 1.02), ('florida', 0.47), ('newsweek', 0.44), ('mitchell', 0.44), ('consent', 0.44)]\n",
      "Topic 31: [('bargaining', 1.11), ('porter', 0.6), ('ohio', 0.58), ('stolar', 0.57), ('wachovia', 0.51)]\n",
      "Topic 32: [('homosexual', 0.8), ('murphy', 0.73), ('overmyer', 0.67), ('committing', 0.67), ('scarpelli', 0.57)]\n",
      "Topic 33: [('louis', 1.31), ('executed', 1.14), ('pipeline', 1.1), ('st', 0.99), ('madison', 0.99)]\n",
      "Topic 34: [('maslenjak', 0.66), ('socket', 0.64), ('professional', 0.62), ('mcwilliams', 0.57), ('clackamas', 0.48)]\n",
      "Topic 35: [('carjacking', 0.95), ('spano', 0.76), ('watson', 0.69), ('multistate', 0.68), ('vmi', 0.55)]\n",
      "Topic 36: [('death', 16.38), ('habeas', 14.48), ('murder', 14.15), ('counsel', 9.87), ('petition', 8.89)]\n",
      "Topic 37: [('robinson', 0.97), ('zimmer', 0.85), ('pulse', 0.85), ('nasa', 0.65), ('halo', 0.64)]\n",
      "Topic 38: [('pacific', 0.66), ('taniguchi', 0.53), ('samantar', 0.52), ('louisiana', 0.51), ('kan', 0.45)]\n",
      "Topic 39: [('currently', 1.6), ('brown', 1.07), ('suspect', 0.83), ('impact', 0.73), ('texas', 0.61)]\n",
      "Topic 40: [('searched', 1.51), ('apartment', 1.11), ('moore', 0.91), ('florida', 0.89), ('fifteen', 0.76)]\n",
      "Topic 41: [('voter', 4.0), ('political', 2.97), ('candidate', 2.67), ('campaign', 2.53), ('primary', 2.29)]\n",
      "Topic 42: [('thompkins', 0.74), ('parochial', 0.69), ('herring', 0.68), ('abel', 0.65), ('insured', 0.62)]\n",
      "Topic 43: [('atkins', 1.31), ('craig', 0.95), ('cake', 0.74), ('mullins', 0.74), ('mentally', 0.66)]\n",
      "Topic 44: [('plan', 6.58), ('erisa', 5.49), ('retirement', 3.58), ('income', 3.27), ('fiduciary', 1.97)]\n",
      "Topic 45: [('garcia', 1.21), ('carney', 1.2), ('represent', 1.01), ('montgomery', 0.87), ('espitia', 0.76)]\n",
      "Topic 46: [('green', 1.12), ('eeo', 0.94), ('pereira', 0.9), ('tanf', 0.79), ('mullenix', 0.68)]\n",
      "Topic 47: [('hampshire', 0.94), ('milk', 0.9), ('region', 0.65), ('camp', 0.56), ('undue', 0.55)]\n",
      "Topic 48: [('working', 2.06), ('eeoc', 1.38), ('disabled', 1.38), ('extortion', 1.16), ('bass', 1.12)]\n",
      "Topic 49: [('deduction', 1.94), ('association', 1.01), ('stock', 0.82), ('income', 0.8), ('ford', 0.76)]\n",
      "Topic 50: [('curtis', 0.68), ('transcript', 0.67), ('alaska', 0.55), ('buchanan', 0.53), ('redhail', 0.52)]\n",
      "Topic 51: [('plea', 1.97), ('judulang', 1.3), ('young', 1.02), ('pled', 0.81), ('crooker', 0.74)]\n",
      "Topic 52: [('promega', 1.04), ('patent', 0.9), ('technology', 0.87), ('lifetech', 0.84), ('closure', 0.81)]\n",
      "Topic 53: [('wheeler', 1.47), ('frazee', 1.23), ('welfare', 0.81), ('covered', 0.65), ('sunday', 0.63)]\n",
      "Topic 54: [('officer', 8.6), ('search', 7.93), ('vehicle', 6.24), ('suppress', 4.56), ('arrest', 4.35)]\n",
      "Topic 55: [('apportionment', 0.88), ('fackrell', 0.6), ('brendlin', 0.55), ('roviaro', 0.54), ('allen', 0.47)]\n",
      "Topic 56: [('morton', 0.62), ('davila', 0.6), ('lee', 0.54), ('cardinal', 0.53), ('interim', 0.52)]\n",
      "Topic 57: [('cigarette', 1.05), ('advertising', 0.99), ('hudson', 0.99), ('burn', 0.67), ('becker', 0.65)]\n",
      "Topic 58: [('minnesota', 1.21), ('membership', 1.2), ('male', 1.01), ('accommodation', 0.83), ('age', 0.81)]\n",
      "Topic 59: [('jeopardy', 2.93), ('double', 2.79), ('punitive', 2.13), ('puerto', 1.41), ('rico', 1.26)]\n",
      "Topic 60: [('woodson', 0.99), ('gregg', 0.98), ('originally', 0.98), ('oracle', 0.95), ('sunday', 0.84)]\n",
      "Topic 61: [('ballot', 1.21), ('begin', 0.65), ('sheehan', 0.6), ('beazer', 0.48), ('conception', 0.48)]\n",
      "Topic 62: [('commerce', 4.91), ('interstate', 3.47), ('icc', 2.26), ('clause', 1.25), ('sims', 1.0)]\n",
      "Topic 63: [('engle', 0.77), ('innis', 0.75), ('nautilus', 0.66), ('facebook', 0.61), ('flood', 0.59)]\n",
      "Topic 64: [('democrat', 0.92), ('navy', 0.7), ('sfts', 0.64), ('ssdi', 0.55), ('truck', 0.48)]\n",
      "Topic 65: [('notification', 1.04), ('method', 0.98), ('contempt', 0.81), ('naacp', 0.76), ('eaja', 0.75)]\n",
      "Topic 66: [('dr', 1.12), ('western', 1.07), ('loo', 1.01), ('contracted', 0.82), ('theory', 0.8)]\n",
      "Topic 67: [('water', 3.65), ('cwa', 1.83), ('wetland', 1.1), ('navigable', 1.07), ('inspection', 0.99)]\n",
      "Topic 68: [('sell', 0.8), ('leaf', 0.72), ('competent', 0.55), ('magistrate', 0.47), ('jones', 0.42)]\n",
      "Topic 69: [('jimenez', 1.21), ('uranium', 0.84), ('woman', 0.51), ('mining', 0.49), ('endrews', 0.44)]\n",
      "Topic 70: [('medtronic', 1.14), ('patent', 0.83), ('hughes', 0.81), ('mfv', 0.63), ('hmt', 0.55)]\n",
      "Topic 71: [('internal', 2.91), ('howard', 1.87), ('code', 1.62), ('equitable', 1.56), ('handgun', 1.05)]\n",
      "Topic 72: [('reform', 0.84), ('burbine', 0.72), ('ifc', 0.69), ('felix', 0.65), ('bozeman', 0.62)]\n",
      "Topic 73: [('seat', 0.93), ('reapportionment', 0.84), ('community', 0.76), ('restitution', 0.63), ('mesa', 0.6)]\n",
      "Topic 74: [('coal', 0.73), ('santanas', 0.73), ('atchley', 0.64), ('marked', 0.62), ('house', 0.61)]\n",
      "Topic 75: [('edison', 0.69), ('indigents', 0.58), ('davis', 0.56), ('dune', 0.54), ('monte', 0.51)]\n",
      "Topic 76: [('possession', 4.97), ('felony', 3.92), ('acca', 2.31), ('violent', 2.24), ('felon', 2.14)]\n",
      "Topic 77: [('shepard', 0.72), ('bradley', 0.64), ('ksr', 0.56), ('ayestas', 0.56), ('gambling', 0.55)]\n",
      "Topic 78: [('partnership', 1.37), ('obscene', 1.1), ('king', 0.94), ('obscenity', 0.92), ('abetting', 0.87)]\n",
      "Topic 79: [('basic', 0.76), ('truck', 0.74), ('marquez', 0.71), ('mitchell', 0.67), ('draper', 0.55)]\n",
      "Topic 80: [('arbitrator', 0.73), ('contraceptive', 0.7), ('ginzburg', 0.64), ('community', 0.64), ('online', 0.59)]\n",
      "Topic 81: [('trust', 3.99), ('trustee', 1.81), ('tribal', 1.75), ('decide', 1.67), ('correctional', 1.66)]\n",
      "Topic 82: [('foot', 0.99), ('sniff', 0.83), ('clinic', 0.78), ('colorado', 0.71), ('wood', 0.69)]\n",
      "Topic 83: [('mutual', 0.95), ('rawlinson', 0.7), ('dirk', 0.69), ('slusa', 0.63), ('corruption', 0.59)]\n",
      "Topic 84: [('hishon', 1.6), ('ncaa', 0.57), ('novo', 0.55), ('mcdonald', 0.51), ('caraco', 0.45)]\n",
      "Topic 85: [('chevron', 1.31), ('quarles', 1.01), ('copy', 0.98), ('doctor', 0.85), ('pharmacy', 0.81)]\n",
      "Topic 86: [('suspended', 0.98), ('iowa', 0.92), ('tovar', 0.68), ('shelton', 0.65), ('supervisor', 0.63)]\n",
      "Topic 87: [('unknown', 0.92), ('pollutant', 0.8), ('reese', 0.73), ('mitchell', 0.66), ('welsh', 0.62)]\n",
      "Topic 88: [('forest', 1.65), ('harvesting', 0.87), ('rehaif', 0.79), ('lawsuit', 0.72), ('chimney', 0.59)]\n",
      "Topic 89: [('cheever', 1.55), ('vickie', 0.65), ('blueford', 0.57), ('radioactive', 0.55), ('ivd', 0.54)]\n",
      "Topic 90: [('iranian', 0.95), ('disciplinary', 0.65), ('gregory', 0.6), ('carter', 0.6), ('baird', 0.55)]\n",
      "Topic 91: [('daca', 1.06), ('medicaid', 0.87), ('social', 0.87), ('thompson', 0.76), ('reimburse', 0.75)]\n",
      "Topic 92: [('institution', 0.98), ('twenty', 0.88), ('educational', 0.81), ('whitfield', 0.71), ('higher', 0.7)]\n",
      "Topic 93: [('trammel', 0.94), ('kaufman', 0.91), ('sioux', 0.89), ('otis', 0.86), ('campbell', 0.78)]\n",
      "Topic 94: [('court', 100.67), ('district', 47.49), ('state', 45.12), ('appeal', 41.55), ('federal', 40.28)]\n",
      "Topic 95: [('ad', 0.81), ('tribal', 0.63), ('brown', 0.62), ('book', 0.45), ('hick', 0.44)]\n",
      "Topic 96: [('heroin', 1.83), ('door', 1.59), ('stored', 0.98), ('bribery', 0.94), ('plea', 0.85)]\n",
      "Topic 97: [('hall', 0.79), ('ca', 0.76), ('bookingcom', 0.64), ('text', 0.59), ('plain', 0.57)]\n",
      "Topic 98: [('arbitration', 8.15), ('immunity', 6.04), ('dismissal', 4.18), ('clause', 4.06), ('reimbursement', 3.58)]\n",
      "Topic 99: [('andersen', 0.68), ('buckhannon', 0.67), ('daily', 0.65), ('advisory', 0.61), ('troy', 0.54)]\n",
      "Topic 100: [('transmission', 0.69), ('brumfield', 0.64), ('tucker', 0.56), ('committee', 0.52), ('ferc', 0.49)]\n",
      "Topic 101: [('estate', 0.91), ('preclearance', 0.9), ('sixmember', 0.76), ('kingsley', 0.72), ('escobedo', 0.69)]\n",
      "Topic 102: [('colorado', 1.1), ('airline', 0.86), ('kent', 0.73), ('ring', 0.73), ('lance', 0.67)]\n",
      "Topic 103: [('angeles', 1.49), ('los', 1.47), ('otcs', 0.75), ('airport', 0.69), ('humphries', 0.64)]\n",
      "Topic 104: [('checkpoint', 1.33), ('lincoln', 0.77), ('gambling', 0.75), ('roadblock', 0.59), ('epcra', 0.57)]\n",
      "Topic 105: [('juror', 1.96), ('selection', 1.84), ('mitchell', 1.31), ('cca', 1.07), ('strike', 1.07)]\n",
      "Topic 106: [('slack', 0.88), ('weaver', 0.79), ('orr', 0.69), ('passport', 0.64), ('shotwell', 0.58)]\n",
      "Topic 107: [('rico', 2.87), ('vendor', 1.43), ('influenced', 1.24), ('racketeer', 1.22), ('corrupt', 1.2)]\n",
      "Topic 108: [('butler', 1.63), ('intoxication', 0.9), ('powell', 0.9), ('alcoholism', 0.77), ('cruel', 0.72)]\n",
      "Topic 109: [('johnson', 1.37), ('patent', 0.94), ('santobello', 0.67), ('glenn', 0.65), ('yates', 0.64)]\n",
      "Topic 110: [('bennett', 1.36), ('taxation', 0.85), ('boycott', 0.79), ('income', 0.77), ('twombly', 0.66)]\n",
      "Topic 111: [('williams', 1.39), ('top', 0.83), ('birth', 0.8), ('moncrieffe', 0.77), ('jerusalem', 0.71)]\n",
      "Topic 112: [('anderson', 0.66), ('town', 0.46), ('medicare', 0.44), ('patient', 0.42), ('liberty', 0.37)]\n",
      "Topic 113: [('mcneely', 0.67), ('matlock', 0.66), ('vinson', 0.54), ('taylor', 0.43), ('hood', 0.38)]\n",
      "Topic 114: [('forney', 0.73), ('carlisle', 0.63), ('stay', 0.54), ('underwriter', 0.54), ('haley', 0.53)]\n",
      "Topic 115: [('limitation', 8.39), ('florida', 6.54), ('michigan', 6.25), ('voting', 5.42), ('parole', 4.88)]\n",
      "Topic 116: [('lewis', 1.93), ('deprived', 1.09), ('idaho', 1.06), ('unavailable', 1.05), ('blakely', 0.94)]\n",
      "Topic 117: [('kiryas', 0.77), ('joel', 0.69), ('sacketts', 0.68), ('boundary', 0.62), ('school', 0.54)]\n",
      "Topic 118: [('varity', 0.97), ('jesus', 0.92), ('christ', 0.91), ('grant', 0.84), ('cl', 0.83)]\n",
      "Topic 119: [('race', 1.05), ('districting', 0.97), ('predominant', 0.74), ('legislative', 0.63), ('boyle', 0.61)]\n",
      "Topic 120: [('partes', 0.75), ('patent', 0.73), ('fsa', 0.7), ('inter', 0.7), ('doggett', 0.69)]\n",
      "Topic 121: [('jimeno', 1.3), ('inside', 0.97), ('wage', 0.8), ('seller', 0.64), ('pocket', 0.6)]\n",
      "Topic 122: [('registration', 1.05), ('provisional', 0.68), ('controversy', 0.67), ('gme', 0.6), ('butz', 0.48)]\n",
      "Topic 123: [('bivens', 1.24), ('iirira', 1.01), ('agent', 0.74), ('unit', 0.66), ('deportation', 0.61)]\n",
      "Topic 124: [('perry', 0.9), ('deference', 0.87), ('booth', 0.71), ('gypsum', 0.63), ('sa', 0.57)]\n",
      "Topic 125: [('immigration', 5.89), ('bia', 3.48), ('permanent', 2.11), ('lawful', 1.47), ('bias', 1.3)]\n",
      "Topic 126: [('riggins', 0.7), ('telephone', 0.65), ('splash', 0.64), ('specie', 0.62), ('evans', 0.59)]\n",
      "Topic 127: [('nixon', 1.91), ('environment', 1.04), ('abusive', 0.83), ('delia', 0.75), ('harassment', 0.6)]\n",
      "Topic 128: [('citizenship', 1.55), ('protest', 1.35), ('leave', 1.28), ('student', 1.26), ('demonstration', 1.22)]\n",
      "Topic 129: [('university', 4.53), ('admission', 4.41), ('council', 3.03), ('application', 2.96), ('call', 2.67)]\n",
      "Topic 130: [('fiore', 0.96), ('gipson', 0.9), ('kirkpatrick', 0.77), ('hawaiian', 0.73), ('walden', 0.7)]\n",
      "Topic 131: [('convincing', 1.0), ('arlington', 0.76), ('flag', 0.59), ('street', 0.58), ('clear', 0.58)]\n",
      "Topic 132: [('naacp', 1.21), ('seed', 0.95), ('monsanto', 0.89), ('chase', 0.87), ('csb', 0.86)]\n",
      "Topic 133: [('broadcasting', 0.82), ('medellin', 0.73), ('zenith', 0.7), ('scholarship', 0.65), ('knudson', 0.55)]\n",
      "Topic 134: [('agent', 1.88), ('fbi', 1.66), ('rehearing', 1.49), ('abel', 1.26), ('banc', 1.07)]\n",
      "Topic 135: [('epa', 1.88), ('naaqs', 0.9), ('boyle', 0.65), ('pollard', 0.63), ('cisco', 0.62)]\n",
      "Topic 136: [('strickland', 1.62), ('moore', 1.5), ('cohen', 1.39), ('patient', 1.27), ('deficient', 1.2)]\n",
      "Topic 137: [('iran', 1.7), ('iranian', 0.94), ('attachment', 0.82), ('rosenbloom', 0.65), ('tyler', 0.64)]\n",
      "Topic 138: [('cos', 0.79), ('kisela', 0.69), ('dai', 0.59), ('satp', 0.49), ('bendix', 0.49)]\n",
      "Topic 139: [('montana', 0.94), ('brogan', 0.72), ('egelhoff', 0.69), ('goldstein', 0.51), ('exxon', 0.5)]\n",
      "Topic 140: [('river', 1.54), ('nation', 1.32), ('claimant', 1.01), ('fuel', 0.78), ('choctaw', 0.69)]\n",
      "Topic 141: [('ship', 1.31), ('cure', 1.02), ('maintenance', 0.84), ('jennings', 0.78), ('vella', 0.69)]\n",
      "Topic 142: [('mccarty', 1.43), ('park', 0.82), ('afroyim', 0.74), ('ohler', 0.65), ('williams', 0.62)]\n",
      "Topic 143: [('segregated', 0.54), ('mlb', 0.46), ('implemented', 0.44), ('denver', 0.4), ('preparation', 0.27)]\n",
      "Topic 144: [('thrift', 1.01), ('dusenbery', 0.86), ('supervisory', 0.78), ('alford', 0.72), ('traveler', 0.68)]\n",
      "Topic 145: [('sheppard', 1.21), ('kyles', 0.68), ('mcdonough', 0.64), ('prejudicial', 0.64), ('montalvo', 0.64)]\n",
      "Topic 146: [('copyright', 3.06), ('miranda', 2.44), ('read', 1.87), ('warning', 1.64), ('game', 1.37)]\n",
      "Topic 147: [('jail', 1.07), ('inmate', 0.91), ('juror', 0.87), ('art', 0.86), ('minority', 0.78)]\n",
      "Topic 148: [('heffernan', 0.74), ('jackson', 0.7), ('flower', 0.64), ('asahi', 0.59), ('morrison', 0.59)]\n",
      "Topic 149: [('abortion', 1.31), ('pennsylvania', 1.03), ('physician', 0.74), ('adoption', 0.63), ('viability', 0.6)]\n",
      "Topic 150: [('lsd', 0.57), ('neal', 0.37), ('sidewalk', 0.33), ('postal', 0.33), ('kokinda', 0.28)]\n",
      "Topic 151: [('film', 2.62), ('obscene', 1.19), ('georgia', 0.93), ('theatre', 0.78), ('spink', 0.72)]\n",
      "Topic 152: [('girl', 1.96), ('allentown', 1.35), ('screen', 0.95), ('marathon', 0.87), ('coy', 0.83)]\n",
      "Topic 153: [('brown', 1.31), ('questioning', 0.95), ('detective', 0.72), ('cobb', 0.71), ('sorrell', 0.55)]\n",
      "Topic 154: [('amgen', 0.56), ('jackson', 0.54), ('batt', 0.51), ('refiner', 0.49), ('impact', 0.48)]\n",
      "Topic 155: [('archer', 0.82), ('hoffman', 0.64), ('title', 0.59), ('beggerly', 0.57), ('clarkstown', 0.56)]\n",
      "Topic 156: [('abortion', 5.13), ('woman', 2.57), ('roe', 1.22), ('aca', 1.08), ('clinic', 0.99)]\n",
      "Topic 157: [('tsakopoulos', 0.62), ('cunningham', 0.61), ('ewing', 0.6), ('thornton', 0.55), ('sunday', 0.5)]\n",
      "Topic 158: [('value', 1.18), ('saving', 0.87), ('mortgage', 0.53), ('sekhar', 0.51), ('abdulkabir', 0.47)]\n",
      "Topic 159: [('university', 0.78), ('doe', 0.67), ('harbert', 0.65), ('rivet', 0.51), ('cornerstone', 0.45)]\n",
      "Topic 160: [('release', 3.6), ('supervised', 1.71), ('dakota', 1.04), ('nevada', 1.03), ('johnson', 1.0)]\n",
      "Topic 161: [('elector', 1.09), ('senate', 0.93), ('offender', 0.9), ('lawton', 0.79), ('hobbie', 0.79)]\n",
      "Topic 162: [('ritzen', 0.78), ('beaudreaux', 0.73), ('byrd', 0.64), ('stewart', 0.57), ('cedar', 0.47)]\n",
      "Topic 163: [('handicapped', 0.84), ('osborn', 0.81), ('doe', 0.79), ('roberson', 0.73), ('device', 0.66)]\n",
      "Topic 164: [('cercla', 0.89), ('davila', 0.82), ('aviall', 0.7), ('apple', 0.6), ('comprehensive', 0.6)]\n",
      "Topic 165: [('glove', 0.94), ('compartment', 0.85), ('mission', 0.8), ('specie', 0.79), ('knight', 0.78)]\n",
      "Topic 166: [('telecommunication', 1.76), ('team', 1.66), ('fcc', 0.83), ('nfl', 0.81), ('customer', 0.68)]\n",
      "Topic 167: [('waste', 1.9), ('bp', 1.18), ('solid', 1.07), ('importation', 0.92), ('tenant', 0.9)]\n",
      "Topic 168: [('kirtsaeng', 1.72), ('textbook', 0.86), ('amgen', 0.66), ('wiley', 0.6), ('thailand', 0.6)]\n",
      "Topic 169: [('pto', 0.86), ('patent', 0.58), ('apache', 0.56), ('hourly', 0.45), ('enhancement', 0.39)]\n",
      "Topic 170: [('tax', 21.21), ('new', 16.35), ('city', 15.98), ('state', 14.98), ('jurisdiction', 14.09)]\n",
      "Topic 171: [('unusual', 3.08), ('cruel', 2.82), ('antiterrorism', 1.77), ('harris', 1.77), ('proposition', 1.26)]\n",
      "Topic 172: [('tribe', 8.64), ('indian', 4.72), ('reservation', 2.95), ('female', 1.68), ('certification', 1.5)]\n",
      "Topic 173: [('scout', 0.69), ('watkins', 0.69), ('field', 0.66), ('alafabco', 0.6), ('transaction', 0.54)]\n",
      "Topic 174: [('director', 2.62), ('appointment', 1.44), ('lake', 1.17), ('board', 1.12), ('portion', 1.09)]\n",
      "Topic 175: [('nature', 0.87), ('motorcycle', 0.63), ('williams', 0.61), ('carter', 0.6), ('closing', 0.55)]\n",
      "Topic 176: [('church', 1.47), ('graham', 1.28), ('lot', 1.18), ('brother', 1.08), ('banned', 0.69)]\n",
      "Topic 177: [('ohio', 2.62), ('minor', 1.63), ('deny', 1.35), ('ex', 1.31), ('parental', 1.28)]\n",
      "Topic 178: [('award', 0.98), ('grc', 0.9), ('marx', 0.9), ('fee', 0.82), ('scheduled', 0.81)]\n",
      "Topic 179: [('ford', 1.33), ('emotional', 1.12), ('distress', 1.01), ('delaware', 0.86), ('ross', 0.82)]\n",
      "Topic 180: [('raisin', 0.63), ('harris', 0.51), ('melin', 0.46), ('sveen', 0.46), ('garza', 0.41)]\n",
      "Topic 181: [('americold', 0.66), ('adam', 0.61), ('pension', 0.57), ('goldfarb', 0.57), ('informer', 0.57)]\n",
      "Topic 182: [('flag', 1.04), ('perkins', 1.03), ('blake', 0.72), ('davis', 0.71), ('cooley', 0.7)]\n",
      "Topic 183: [('mohawk', 1.44), ('immediately', 1.18), ('prepaid', 0.88), ('florida', 0.87), ('college', 0.84)]\n",
      "Topic 184: [('lonchar', 1.07), ('pepper', 0.68), ('summer', 0.65), ('ohralik', 0.65), ('lonchars', 0.64)]\n",
      "Topic 185: [('scott', 1.03), ('debtor', 0.73), ('parole', 0.71), ('crown', 0.6), ('business', 0.48)]\n",
      "Topic 186: [('secret', 1.37), ('dependent', 1.36), ('leon', 1.1), ('dilution', 1.06), ('tip', 0.84)]\n",
      "Topic 187: [('production', 1.35), ('deported', 1.11), ('subsection', 1.1), ('deportation', 1.03), ('punishable', 0.84)]\n",
      "Topic 188: [('red', 0.58), ('west', 0.56), ('prosecuted', 0.54), ('emilys', 0.52), ('ethic', 0.49)]\n",
      "Topic 189: [('street', 1.2), ('near', 1.17), ('location', 1.09), ('ina', 1.05), ('abstain', 0.96)]\n",
      "Topic 190: [('inference', 0.84), ('huff', 0.73), ('selective', 0.71), ('mexican', 0.58), ('haley', 0.58)]\n",
      "Topic 191: [('bankruptcy', 9.08), ('chapter', 3.02), ('creditor', 1.46), ('code', 1.43), ('nelson', 1.36)]\n",
      "Topic 192: [('facie', 1.27), ('prima', 1.26), ('horowitz', 0.71), ('mentally', 0.71), ('retarded', 0.65)]\n",
      "Topic 193: [('solicitation', 1.04), ('farmer', 1.01), ('cancellation', 0.78), ('hospital', 0.75), ('muniz', 0.75)]\n",
      "Topic 194: [('ftca', 2.49), ('tort', 1.97), ('performance', 1.78), ('liable', 1.71), ('providing', 1.58)]\n",
      "Topic 195: [('gray', 0.78), ('patten', 0.65), ('tazewell', 0.59), ('minton', 0.54), ('van', 0.52)]\n",
      "Topic 196: [('sca', 0.74), ('bankruptcy', 0.62), ('ibm', 0.56), ('depot', 0.55), ('ayala', 0.54)]\n",
      "Topic 197: [('foreign', 2.39), ('fsia', 1.62), ('spencer', 1.52), ('carrying', 1.36), ('sovereign', 1.31)]\n",
      "Topic 198: [('school', 20.32), ('student', 8.14), ('education', 6.11), ('teacher', 4.75), ('private', 2.71)]\n",
      "Topic 199: [('system', 4.62), ('social', 3.89), ('litigation', 2.85), ('began', 2.63), ('termination', 2.46)]\n",
      "Topic 200: [('stock', 2.88), ('sec', 2.67), ('investor', 2.0), ('asset', 1.91), ('class', 1.46)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data\n",
       "1       698\n",
       "0       239\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 65.31%\n",
      "f1_score : 0.7660187185025199\n"
     ]
    }
   ],
   "source": [
    "data_final = chooseVec('tf', concatSample())\n",
    "fullTraining(data_final)\n",
    "prediction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict(data_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_party_winner\n",
       "1                     1040\n",
       "0                      200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predcsv.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
